{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77av3yufgc",
   "metadata": {},
   "source": [
    "### Comparación e Identificación de Voz\n",
    "\n",
    "Identificar un hablante comparando su voz contra las voces registradas en la base de datos.\n",
    "\n",
    "**Pasos:** Configurar rutas → Cargar modelo → Grabar audio → Generar embedding → Cargar BD → Calcular similitud → Identificar (threshold 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joxjmpmyh1",
   "metadata": {},
   "source": [
    "#### 1. Validar modelo y configurar rutas\n",
    "Verifica que el modelo existe y define directorio base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64c0f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo listo en: d:\\work_jhonatan_becerra\\speach-recognition\\models\\spkrec-ecapa-voxceleb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = os.path.abspath(\"..\")\n",
    "\n",
    "MODEL_DIR = os.path.join(\n",
    "    BASE_DIR,\n",
    "    \"models\",\n",
    "    \"spkrec-ecapa-voxceleb\"\n",
    ")\n",
    "\n",
    "assert os.path.exists(MODEL_DIR), \"Modelo no encontrado\"\n",
    "assert os.path.exists(os.path.join(MODEL_DIR, \"hyperparams.yaml\")), \"hyperparams.yaml faltante\"\n",
    "\n",
    "print(\"Modelo listo en:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrs7ed7a3h",
   "metadata": {},
   "source": [
    "#### 2. Cargar modelo ECAPA-TDNN\n",
    "Carga el modelo SpeechBrain con directorio cache temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f49f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\work_jhonatan_becerra\\speach-recognition\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "d:\\work_jhonatan_becerra\\speach-recognition\\venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:22: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "d:\\work_jhonatan_becerra\\speach-recognition\\venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:22: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(\n",
    "    source=MODEL_DIR,\n",
    "    savedir=os.path.join(MODEL_DIR, \"_cache\"),\n",
    "    run_opts={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "print(\"Modelo cargado correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1uw5el4r6o",
   "metadata": {},
   "source": [
    "#### 3. Configurar parámetros de captura\n",
    "16 kHz, mono, 10 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "SAMPLE_RATE = 16_000\n",
    "CHANNELS = 1\n",
    "DURATION_SEC = 10\n",
    "\n",
    "AUDIO_DIR = os.path.join(BASE_DIR, \"audio\")\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "\n",
    "AUDIO_PATH = os.path.join(AUDIO_DIR, \"audio_10s.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ve3wam7jgih",
   "metadata": {},
   "source": [
    "#### 4. Grabar audio del hablante a identificar\n",
    "Captura 10 segundos de audio desde el micrófono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a64881b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabando audio por 10 segundos...\n",
      "Audio guardado en: d:\\work_jhonatan_becerra\\speach-recognition\\audio\\audio_10s.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "print(\"Grabando audio por 10 segundos...\")\n",
    "audio = sd.rec(\n",
    "    int(DURATION_SEC * SAMPLE_RATE),\n",
    "    samplerate=SAMPLE_RATE,\n",
    "    channels=CHANNELS,\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "sd.wait()\n",
    "\n",
    "sf.write(AUDIO_PATH, audio, SAMPLE_RATE)\n",
    "print(\"Audio guardado en:\", AUDIO_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ywxiadclha8",
   "metadata": {},
   "source": [
    "#### 5. Procesar y normalizar waveform\n",
    "Valida sample rate y duración, convierte a mono y normaliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9687d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform shape: torch.Size([1, 160000])\n"
     ]
    }
   ],
   "source": [
    "waveform, sr = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "assert sr == SAMPLE_RATE, f\"Sample rate inválido: {sr}\"\n",
    "assert waveform.shape[1] >= SAMPLE_RATE * DURATION_SEC * 0.95, \"Audio demasiado corto\"\n",
    "\n",
    "waveform = waveform.mean(dim=0, keepdim=True)  # mono\n",
    "waveform = waveform / torch.max(torch.abs(waveform))  # normalización\n",
    "\n",
    "print(\"Waveform shape:\", waveform.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uir5vnurzts",
   "metadata": {},
   "source": [
    "#### 6. Generar embedding del audio capturado\n",
    "Extrae vector de 192 dimensiones del audio a identificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e718ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generado. Shape: (192,)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = classifier.encode_batch(waveform)\n",
    "\n",
    "embedding_actual = embedding.squeeze().cpu().numpy()\n",
    "\n",
    "print(\"Embedding generado. Shape:\", embedding_actual.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yeqzhdgjl8",
   "metadata": {},
   "source": [
    "#### 7. Cargar base de datos de voces\n",
    "Lee todos los archivos .npy de voices_db (cada archivo = una persona registrada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c98c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voces cargadas: ['freddy', 'jhonatan']\n"
     ]
    }
   ],
   "source": [
    "VOICES_DB = os.path.join(BASE_DIR, \"voices_db\")\n",
    "os.makedirs(VOICES_DB, exist_ok=True)\n",
    "\n",
    "voice_pool = {}\n",
    "\n",
    "for file in os.listdir(VOICES_DB):\n",
    "    if file.endswith(\".npy\"):\n",
    "        name = file.replace(\".npy\", \"\")\n",
    "        voice_pool[name] = np.load(os.path.join(VOICES_DB, file))\n",
    "\n",
    "print(\"Voces cargadas:\", list(voice_pool.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ewexqv6urwd",
   "metadata": {},
   "source": [
    "#### 8. Calcular similitud coseno\n",
    "Compara embedding actual vs todos los registrados (valores cercanos a 1 = misma persona)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd3f14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freddy': 0.16063125, 'jhonatan': 0.8501665}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for name, emb in voice_pool.items():\n",
    "    scores[name] = cosine_similarity(embedding_actual, emb)\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mnvk8jng7kg",
   "metadata": {},
   "source": [
    "#### 9. Identificar hablante con threshold\n",
    "Score >= 0.75 = identifica persona | Score < 0.75 = desconocido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cdb5430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDENTIDAD: jhonatan (score=0.850)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 0.75\n",
    "\n",
    "if scores:\n",
    "    best_match = max(scores, key=scores.get)\n",
    "    best_score = scores[best_match]\n",
    "\n",
    "    if best_score >= THRESHOLD:\n",
    "        result = f\"IDENTIDAD: {best_match} (score={best_score:.3f})\"\n",
    "    else:\n",
    "        result = f\"DESCONOCIDO (mejor score={best_score:.3f})\"\n",
    "else:\n",
    "    result = \"BD VACÍA – no hay voces registradas\"\n",
    "\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
